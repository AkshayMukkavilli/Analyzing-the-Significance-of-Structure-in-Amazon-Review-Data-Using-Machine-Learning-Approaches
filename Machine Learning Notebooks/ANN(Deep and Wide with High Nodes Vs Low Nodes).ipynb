{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to predict the values without scaling the data and observe if it is helping us in minimizing the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Helpful Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>565</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>3087.000000</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>343</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>468.500000</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>730</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>394.272727</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>194</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>91</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0      3    565           1              0                      3   \n",
       "1      5    162           3              4                      3   \n",
       "2      4    343           4              6                      4   \n",
       "3      4    730          11             20                      3   \n",
       "4      5    194           2              1                      6   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Helpful Votes  \n",
       "0                     93                   3087.000000            837  \n",
       "1                     91                    300.000000            374  \n",
       "2                     90                    468.500000            263  \n",
       "3                     91                    394.272727            200  \n",
       "4                     91                    492.000000            247  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'C:\\Users\\aksha\\PycharmProjects\\FinalProject_WorkingCopy\\FinalFeatures.csv')\n",
    "\n",
    "# The below line of code is to remove all the columns with z-scores from the dataset and move the Helpful Votes \n",
    "# column to the end\n",
    "dataset = dataset[['Stars','Words', 'Paragraphs','No.break tags','Percentage_Upper_Case','Percentage_Lower_Case','Avg_len_paragraph_per_review','Helpful Votes']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the independant variables from the dependant variable which is \"Helpful Votes\" in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:-1].values\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Splitting the data into training data and testing data\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2,random_state=0)\n",
    "\n",
    "# importing keras and other required functions\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 148us/step - loss: 2111.0131\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 3s 140us/step - loss: 2102.0405\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 3s 136us/step - loss: 2125.9705\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 3s 135us/step - loss: 2078.2277\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 3s 136us/step - loss: 2109.1652\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2090.48020s - loss: 2157.\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2093.8703\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2079.5852 ETA: 0s - loss: 2116.\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2012.2606 ETA: 0s -\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2118.91910s - - E\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 3s 128us/step - loss: 2049.1320\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2088.4952\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2079.5688\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2054.4552\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2071.3144\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2104.8318\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2074.8889\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2069.7967\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2075.67660s - los\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2089.43150s - loss: 1\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2035.5529\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2096.3046\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2061.1405\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2111.58950s - loss: 2\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 3s 129us/step - loss: 2084.18591s - loss: 2706.76 - ETA:  - ETA: 0s - loss: - ETA - ETA: 0s - loss: 2076.03\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2051.77011s - loss: 1416.16 - - ETA: 0s - l\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2071.0638\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2081.5608\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2098.23580s - l\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2104.3490\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2041.4875TA: 1s - l - ETA: 1s - loss: 2911.95 - E - ETA\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2053.1400- E\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 3s 129us/step - loss: 2052.47090s - loss: 2071.59\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 3s 129us/step - loss: 2063.3198\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2048.5730\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 3s 129us/step - loss: 2083.7136\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 3s 138us/step - loss: 2013.8024\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2053.3573\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2041.22970s - los\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2096.60530s - loss: 2099.75\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2088.3406\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2033.9502\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2084.50370s - loss:\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 1997.2911\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2052.4241\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2081.1126\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 3s 129us/step - loss: 2078.5510\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2042.5464\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2047.9564\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2101.6050\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2043.5945\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2087.4703\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2084.83012 - ETA: 2s - los - ETA: 1s - loss:  - ETA: 1s - loss: 953\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 3s 137us/step - loss: 2038.6506\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2042.0191\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 3s 135us/step - loss: 2048.3686\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 4s 147us/step - loss: 2034.5556\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 2056.2806\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 3s 137us/step - loss: 2041.9615\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 3s 137us/step - loss: 2026.6967\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 3s 133us/step - loss: 2067.2962\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2082.53061s -\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 3s 133us/step - loss: 2067.22230s - loss: 1\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2107.2051\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2058.84530s \n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2034.9310\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2057.6872\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 2031.2630\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2029.4157\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2088.0969\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 3s 133us/step - loss: 2032.5139\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2057.4320\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 3s 135us/step - loss: 2021.3820\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2012.3187\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 3s 138us/step - loss: 1987.95650s - loss: 2045.\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2007.2523\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 3s 133us/step - loss: 2040.81320s - loss: 1967.60 - ETA\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 144us/step - loss: 1992.8348\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 3s 136us/step - loss: 2057.3638\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 3s 135us/step - loss: 2034.47001s - loss: 3465. - ETA: 0s - loss: 2498. - E\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 2066.19270s - l\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 2023.57640s - loss: 1\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 2055.0591\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2063.8573\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2070.6568\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2044.7152\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 3s 126us/step - loss: 1996.0744\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2019.0911\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2045.03030s - loss: 212\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2085.2547: 0s\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2054.4467\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2075.0038\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2054.66080s -\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2009.11150s - loss:\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 3s 126us/step - loss: 2057.1363\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 3s 130us/step - loss: 2041.1589\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 3s 140us/step - loss: 2027.6811\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 3s 139us/step - loss: 2005.5258\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 3s 139us/step - loss: 2039.3057\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 3s 131us/step - loss: 2041.91300s - los - ETA: 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b8d0d2dd8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a 3 layer ANN with high number of nodes \"\"\"\n",
    "\n",
    "regressor = Sequential()\n",
    "regressor.add(Dense(100, kernel_initializer = 'normal',activation = 'relu',input_dim = 7))\n",
    "regressor.add(Dense(100, kernel_initializer = 'normal', activation = 'relu'))\n",
    "regressor.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>2</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>7</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>8</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>2</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>33</td>\n",
       "      <td>211.582733</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>2</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>0</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>1</td>\n",
       "      <td>1.212858</td>\n",
       "      <td>19.278724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values  Mean Squared Error\n",
       "0                 1          1.212858           19.278724\n",
       "1                 0          1.212858           19.278724\n",
       "2                 0          1.212858           19.278724\n",
       "3                 0          1.212858           19.278724\n",
       "4                 0          1.212858           19.278724\n",
       "5                 0          1.212858           19.278724\n",
       "6                 0          1.212858           19.278724\n",
       "7                 0          1.212858           19.278724\n",
       "8                 0          1.212858           19.278724\n",
       "9                 1          1.212858           19.278724\n",
       "10                0          1.212858           19.278724\n",
       "11                1          1.212858           19.278724\n",
       "12                1          1.212858           19.278724\n",
       "13                0          1.212858           19.278724\n",
       "14                0          1.212858           19.278724\n",
       "15                0          1.212858           19.278724\n",
       "16                1          1.212858           19.278724\n",
       "17                1          1.212858           19.278724\n",
       "18                0          1.212858           19.278724\n",
       "19                0          1.212858           19.278724\n",
       "20                0          1.212858           19.278724\n",
       "21               12          1.212858           19.278724\n",
       "22                1          1.212858           19.278724\n",
       "23                0          1.212858           19.278724\n",
       "24                0          1.212858           19.278724\n",
       "25                0          1.212858           19.278724\n",
       "26                0          1.212858           19.278724\n",
       "27                0          1.212858           19.278724\n",
       "28                0          1.212858           19.278724\n",
       "29                0          1.212858           19.278724\n",
       "...             ...               ...                 ...\n",
       "6041              0          1.212858           19.278724\n",
       "6042              2          1.212858           19.278724\n",
       "6043              7          1.212858           19.278724\n",
       "6044              0          1.212858           19.278724\n",
       "6045              0          1.212858           19.278724\n",
       "6046              0          1.212858           19.278724\n",
       "6047              0          1.212858           19.278724\n",
       "6048              8          1.212858           19.278724\n",
       "6049              0          1.212858           19.278724\n",
       "6050              0          1.212858           19.278724\n",
       "6051              0          1.212858           19.278724\n",
       "6052              0          1.212858           19.278724\n",
       "6053              1          1.212858           19.278724\n",
       "6054              0          1.212858           19.278724\n",
       "6055              0          1.212858           19.278724\n",
       "6056              0          1.212858           19.278724\n",
       "6057              0          1.212858           19.278724\n",
       "6058              2          1.212858           19.278724\n",
       "6059              0          1.212858           19.278724\n",
       "6060              0          1.212858           19.278724\n",
       "6061              0          1.212858           19.278724\n",
       "6062              1          1.212858           19.278724\n",
       "6063             33        211.582733           19.278724\n",
       "6064              1          1.212858           19.278724\n",
       "6065              2          1.212858           19.278724\n",
       "6066              0          1.212858           19.278724\n",
       "6067              0          1.212858           19.278724\n",
       "6068              0          1.212858           19.278724\n",
       "6069              1          1.212858           19.278724\n",
       "6070              1          1.212858           19.278724\n",
       "\n",
       "[6071 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred = y_pred.reshape(6091,)\n",
    "temp = {'Actual Values': y_test,'Predicted Values': y_pred}\n",
    "y_compare = pd.DataFrame(temp)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "y_compare['Mean Squared Error'] = ((np.diff(y_compare.values) ** 2).mean() ** .5)\n",
    "y_compare.head(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already tried an ANN with high number of nodes, we will now test it with lesser number of nodes\n",
    "No. of nodes in each layer = Avg(No. of o/p nodes, No. of i/p nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 2112.9985\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2085.97872s - ETA: 1s - - ETA: 0s - ETA: 0s\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 3s 120us/step - loss: 2076.4402 - ETA: \n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 3s 120us/step - loss: 2075.9922\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2061.8207\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2054.88860s - loss: 1867. - ETA: 0s - los\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2038.5185\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 3s 118us/step - loss: 2069.87620s - \n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 3s 117us/step - loss: 2052.81160s - loss: 1685.\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2051.1699 ETA: 0s - loss: 1\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2052.2304\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 3s 120us/step - loss: 2050.20022s - loss: 2298. - ETA: 1s - ETA: 1s - loss: - ETA - ETA: 0s - loss:\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2049.1766\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2044.7403\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 3s 112us/step - loss: 2047.41251s - loss: 4 - ETA: 1s - loss: 4 - ETA: 1s - ETA: 0s - l - ETA: 0s -\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2038.65701s\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2058.8792: 1s - loss: 5 - ETA: 1s - loss: - E\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 3s 116us/step - loss: 2038.97660s - loss: 2053.71\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2049.8131 1s - loss: - ETA\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2049.23710s - lo\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2046.52360s - l - ETA: 0s - ETA: 0s - loss: 2114.\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2039.3042\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 3s 118us/step - loss: 2048.0426\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2043.26450s - los\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2050.3266\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2048.12160s - loss: 2067.92\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 3s 120us/step - loss: 2011.6174\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2075.5089\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2043.6491\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2048.8386\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 3s 116us/step - loss: 2029.1338\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2076.5463\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2058.40620s - loss: 2114.\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2044.1593\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2047.96622s - loss: 163.073 - ETA: 2s - l - ETA: 1s - loss: 445.6 - ETA: 1s  - ETA: 0s - - ETA: 0s - loss: 2183.80 - ETA: 0s - loss: 214\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 3s 132us/step - loss: 2045.4410\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2064.16551s - lo - E\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 3s 118us/step - loss: 2054.4931\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 3s 118us/step - loss: 2061.24170s - loss: 2\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2055.1940\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 3s 120us/step - loss: 2046.0465\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 3s 117us/step - loss: 2055.75501s - E - ETA: 0s - loss: 1124. - ETA: 0s - loss:\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2037.6980 - ETA: 0s - los\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2045.5333\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2038.6119\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2049.0155 - ETA:  - ETA: 1s - - ETA: 1s -\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2064.9036\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 3s 126us/step - loss: 2035.8385\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2048.53650s -\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 3s 129us/step - loss: 2046.2766\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2037.8579\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2049.8099- ETA: 0s -\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2044.1568\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 3s 118us/step - loss: 2034.6103\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 3s 126us/step - loss: 2045.7317\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2048.34770s - loss: 2189.52 - ETA: 0s - loss: 2\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2032.1948\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2063.7373\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2062.3865 ETA: 0s - loss: 1054.42 - ETA: 0s - los - ETA: \n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2034.11760s - loss: 2047.71\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2040.5224 ETA: 0s - loss: 2046.97\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2034.7562\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2060.1172- ETA: \n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 3s 118us/step - loss: 2054.79332s - lo - ETA: 1s - loss: 79 - ET - ETA:  - ETA: 0s - loss: 2104.\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 3s 127us/step - loss: 2038.2107\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2047.0023\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2049.6066\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 3s 126us/step - loss: 2017.2883\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2052.9652\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2035.43370s - los\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2033.5349\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2026.2993\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2043.38290s - loss: 2673. - ETA: 0s - los - ETA: 0s - loss: 2328. - ETA: 0s - loss: 2243.41 - ETA: 0s - l\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2025.6564\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 3s 123us/step - loss: 2074.2635\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2046.9299\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 3s 124us/step - loss: 2066.53310s - loss: 1125.76 - ETA: 0s - loss: 1105. - ETA: 0s - loss: 106\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2054.4576\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2030.2904TA: 0s - loss: 2470. - ETA: 0s - loss: - ETA: 0s - loss:\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2056.25630s -\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2018.8244\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2032.2806\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2057.9990\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2036.55222s - loss: 5400.68 - ETA: 2s - loss: 4948. -\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2049.46742s - loss: - ETA: \n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2035.40921s - loss: 239 - ETA:  - - ETA: 0s - loss: 215\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2035.0898\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2039.3331\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2028.7363\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2022.03282s - los - ETA: 1s - ETA:  - ETA: 0s - loss: 2586.\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2043.35991s - l\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 3s 113us/step - loss: 2012.3840 - lo - ETA: 1s -\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 3s 112us/step - loss: 2017.3806\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 3s 114us/step - loss: 2068.7858TA: 0s - loss: 2079.81\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 3s 119us/step - loss: 2059.0777\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 3s 125us/step - loss: 2078.1020\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2028.9642\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 3s 121us/step - loss: 2041.38431s\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 3s 122us/step - loss: 2033.4566\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 3s 126us/step - loss: 2039.93790s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b8d75af98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a 3 layer ANN with less number of nodes \"\"\"\n",
    "\n",
    "regressor_1 = Sequential()\n",
    "regressor_1.add(Dense(4, kernel_initializer = 'normal',activation = 'relu',input_dim = 7))\n",
    "regressor_1.add(Dense(4, kernel_initializer = 'normal', activation = 'relu'))\n",
    "regressor_1.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor_1.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor_1.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>3.224155</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>2.966307</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1.362391</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>2</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>7</td>\n",
       "      <td>4.524185</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>8</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0</td>\n",
       "      <td>1.105093</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>2</td>\n",
       "      <td>3.630808</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>33</td>\n",
       "      <td>107.996567</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>1</td>\n",
       "      <td>1.454858</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>2</td>\n",
       "      <td>1.203787</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>0</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>1</td>\n",
       "      <td>0.308648</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>1</td>\n",
       "      <td>0.704455</td>\n",
       "      <td>15.56264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values  Mean Squared Error\n",
       "0                 1          0.308648            15.56264\n",
       "1                 0          0.308648            15.56264\n",
       "2                 0          0.308648            15.56264\n",
       "3                 0          0.308648            15.56264\n",
       "4                 0          0.308648            15.56264\n",
       "5                 0          0.308648            15.56264\n",
       "6                 0          0.308648            15.56264\n",
       "7                 0          0.308648            15.56264\n",
       "8                 0          0.308648            15.56264\n",
       "9                 1          0.308648            15.56264\n",
       "10                0          0.308648            15.56264\n",
       "11                1          0.308648            15.56264\n",
       "12                1          0.308648            15.56264\n",
       "13                0          0.308648            15.56264\n",
       "14                0          0.308648            15.56264\n",
       "15                0          0.308648            15.56264\n",
       "16                1          0.308648            15.56264\n",
       "17                1          3.224155            15.56264\n",
       "18                0          0.308648            15.56264\n",
       "19                0          0.308648            15.56264\n",
       "20                0          0.308648            15.56264\n",
       "21               12          2.966307            15.56264\n",
       "22                1          0.308648            15.56264\n",
       "23                0          0.308648            15.56264\n",
       "24                0          0.308648            15.56264\n",
       "25                0          0.308648            15.56264\n",
       "26                0          0.308648            15.56264\n",
       "27                0          1.362391            15.56264\n",
       "28                0          0.308648            15.56264\n",
       "29                0          0.308648            15.56264\n",
       "...             ...               ...                 ...\n",
       "6041              0          0.308648            15.56264\n",
       "6042              2          0.308648            15.56264\n",
       "6043              7          4.524185            15.56264\n",
       "6044              0          0.308648            15.56264\n",
       "6045              0          0.308648            15.56264\n",
       "6046              0          0.308648            15.56264\n",
       "6047              0          0.308648            15.56264\n",
       "6048              8          0.308648            15.56264\n",
       "6049              0          0.308648            15.56264\n",
       "6050              0          0.308648            15.56264\n",
       "6051              0          0.308648            15.56264\n",
       "6052              0          1.105093            15.56264\n",
       "6053              1          0.308648            15.56264\n",
       "6054              0          0.308648            15.56264\n",
       "6055              0          0.308648            15.56264\n",
       "6056              0          0.308648            15.56264\n",
       "6057              0          0.308648            15.56264\n",
       "6058              2          3.630808            15.56264\n",
       "6059              0          0.308648            15.56264\n",
       "6060              0          0.308648            15.56264\n",
       "6061              0          0.308648            15.56264\n",
       "6062              1          0.308648            15.56264\n",
       "6063             33        107.996567            15.56264\n",
       "6064              1          1.454858            15.56264\n",
       "6065              2          1.203787            15.56264\n",
       "6066              0          0.308648            15.56264\n",
       "6067              0          0.308648            15.56264\n",
       "6068              0          0.308648            15.56264\n",
       "6069              1          0.308648            15.56264\n",
       "6070              1          0.704455            15.56264\n",
       "\n",
       "[6071 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred_1 = regressor_1.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred_1 = y_pred_1.reshape(6091,)\n",
    "temp_1 = {'Actual Values': y_test,'Predicted Values': y_pred_1}\n",
    "y_compare_1 = pd.DataFrame(temp_1)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "y_compare_1['Mean Squared Error'] = ((np.diff(y_compare_1.values) ** 2).mean() ** .5)\n",
    "y_compare_1.head(-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try a wide network with large number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 6s 233us/step - loss: 2124.6449\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 5s 212us/step - loss: 2118.9085\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2126.0773\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2118.9125\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2113.1253\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 5s 219us/step - loss: 2135.0589\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 5s 216us/step - loss: 2108.7559\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 6s 261us/step - loss: 2091.0594\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 5s 217us/step - loss: 2129.8874\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 6s 231us/step - loss: 2119.1080\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 5s 221us/step - loss: 2170.1149\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 5s 224us/step - loss: 2130.8999\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 5s 223us/step - loss: 2147.8871\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 5s 219us/step - loss: 2124.0909\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 6s 230us/step - loss: 2706.8121\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 5s 225us/step - loss: 2110.3296\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 6s 235us/step - loss: 2114.3144\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 6s 235us/step - loss: 2131.0618\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 5s 222us/step - loss: 2115.3633\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 6s 232us/step - loss: 2120.2602\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 6s 233us/step - loss: 2202.9661\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 5s 222us/step - loss: 2128.2488\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 6s 238us/step - loss: 2119.8733\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 6s 238us/step - loss: 73816.3931\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 5s 219us/step - loss: 2112.0810\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 6s 248us/step - loss: 2247.3623\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 6s 253us/step - loss: 2111.6277\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 7s 283us/step - loss: 2114.3302\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 8s 313us/step - loss: 2094.0775\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 6s 229us/step - loss: 2110.9747\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 5s 211us/step - loss: 2382.6642\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 5s 211us/step - loss: 2152.6777\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 5s 216us/step - loss: 10106.2676\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 5s 210us/step - loss: 2119.5774\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 5s 212us/step - loss: 2130.1988\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2155.7047\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 5s 211us/step - loss: 2136.0179\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 5s 212us/step - loss: 11740.7488\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 5s 210us/step - loss: 2120.8734\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 5s 211us/step - loss: 2103.7777\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 5s 211us/step - loss: 2153.3080\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 5s 219us/step - loss: 2177.2208\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 6s 263us/step - loss: 6524.7245\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 6s 255us/step - loss: 2140.42670s - loss:\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 6s 239us/step - loss: 2150.6707\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 5s 220us/step - loss: 138766.7495\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 5s 222us/step - loss: 2174.6764\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 6s 242us/step - loss: 2141.0242\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 6s 230us/step - loss: 2170.3321\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 5s 220us/step - loss: 2182.2860\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 5s 220us/step - loss: 2284.9598\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 5s 220us/step - loss: 2367.5983\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 5s 220us/step - loss: 2147.4150\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 6s 244us/step - loss: 119635.4912\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 6s 242us/step - loss: 2157.4868\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 6s 232us/step - loss: 2137.2408\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 6s 235us/step - loss: 2117.5521\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 6s 238us/step - loss: 2280.1882\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 5s 222us/step - loss: 2133.3368\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 6s 230us/step - loss: 2222.8197\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2467.4571\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 6s 233us/step - loss: 13819.6718\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 6s 248us/step - loss: 2155.5933\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 5s 217us/step - loss: 2150.9986\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 6s 231us/step - loss: 2150.5210\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 5s 217us/step - loss: 2501.2141\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 5s 218us/step - loss: 2124.8151\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 5s 215us/step - loss: 2349.2930\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 5s 219us/step - loss: 2273.4274\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2132.0315\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2988.5512\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 5s 216us/step - loss: 2121.7801\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 5s 213us/step - loss: 2198.3728\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 5s 226us/step - loss: 2135.3700\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 5s 215us/step - loss: 164579.1002\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 6s 266us/step - loss: 2097.3135\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 7s 291us/step - loss: 2121.6310\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 7s 277us/step - loss: 43198.8360\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 6s 258us/step - loss: 2108.0736\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 6s 233us/step - loss: 2102.5802\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 6s 233us/step - loss: 2136.0806\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 6s 246us/step - loss: 2179.4782\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 6s 235us/step - loss: 2114.4642\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 6s 241us/step - loss: 3340.1846\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 6s 260us/step - loss: 2096.6946\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 5s 219us/step - loss: 2112.2097\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 6s 236us/step - loss: 2802.6283\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 6s 243us/step - loss: 519763.2450\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 6s 248us/step - loss: 2093.1864\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 6s 242us/step - loss: 2096.2003\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 5s 217us/step - loss: 2096.6435\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 6s 231us/step - loss: 2107.7553\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 6s 251us/step - loss: 2112.9831\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 6s 243us/step - loss: 1057080.1422\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 5s 225us/step - loss: 2098.8479\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 5s 225us/step - loss: 2109.7239\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 6s 258us/step - loss: 2125.1920\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 6s 254us/step - loss: 2158.0920\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 6s 229us/step - loss: 2258.0033\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 6s 239us/step - loss: 2129.6690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b8ec638d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Building a deep and wide ANN with high number of nodes \"\"\"\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "regressor_2 = Sequential()\n",
    "regressor_2.add(Dense(50, kernel_initializer = 'normal',input_dim = 7))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(100, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(200, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(100, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(50, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(20, kernel_initializer = 'normal'))\n",
    "regressor_2.add(LeakyReLU(alpha=0.05))\n",
    "regressor_2.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "\"\"\" Compiling the regressor \"\"\"\n",
    "regressor_2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "\"\"\" Fitting the Artifilial Neural Network to our training data \"\"\"\n",
    "regressor_2.fit(X_train, y_train, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Predicted Values</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.065479</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.917383</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.149472</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.323487</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.441041</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2.749945</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.614756</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.977566</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.800737</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1.285007</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1.392544</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>3.259796</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.433205</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1.280274</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1.395333</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1.175252</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1.398108</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>10.723013</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>2.037565</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1.503063</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1.670454</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>10.042000</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.973671</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1.550231</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1.239094</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.745692</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.938713</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>4.339663</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1.682943</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1.427350</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>0</td>\n",
       "      <td>1.560628</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>2</td>\n",
       "      <td>1.508284</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>7</td>\n",
       "      <td>8.568823</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0</td>\n",
       "      <td>0.953897</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0</td>\n",
       "      <td>1.621893</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>0</td>\n",
       "      <td>0.652921</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>0</td>\n",
       "      <td>0.691062</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6048</th>\n",
       "      <td>8</td>\n",
       "      <td>0.509152</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0</td>\n",
       "      <td>2.251305</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0</td>\n",
       "      <td>1.482157</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>0</td>\n",
       "      <td>0.899055</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0</td>\n",
       "      <td>1.725007</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>1</td>\n",
       "      <td>1.249538</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0</td>\n",
       "      <td>0.954063</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>0</td>\n",
       "      <td>1.788742</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>0</td>\n",
       "      <td>1.358596</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>0</td>\n",
       "      <td>1.584856</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>2</td>\n",
       "      <td>2.947666</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6059</th>\n",
       "      <td>0</td>\n",
       "      <td>0.914681</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>0</td>\n",
       "      <td>0.788576</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>0</td>\n",
       "      <td>1.389155</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>1</td>\n",
       "      <td>1.737977</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>33</td>\n",
       "      <td>74.631020</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>1</td>\n",
       "      <td>1.776015</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>2</td>\n",
       "      <td>4.810035</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0</td>\n",
       "      <td>1.064351</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>0</td>\n",
       "      <td>1.370239</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>0</td>\n",
       "      <td>1.368928</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>1</td>\n",
       "      <td>0.916928</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>1</td>\n",
       "      <td>1.282302</td>\n",
       "      <td>15.405691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6071 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Values  Predicted Values  Mean Squared Error\n",
       "0                 1          1.065479           15.405691\n",
       "1                 0          0.917383           15.405691\n",
       "2                 0          1.149472           15.405691\n",
       "3                 0          1.323487           15.405691\n",
       "4                 0          1.441041           15.405691\n",
       "5                 0          2.749945           15.405691\n",
       "6                 0          0.614756           15.405691\n",
       "7                 0          0.977566           15.405691\n",
       "8                 0          0.800737           15.405691\n",
       "9                 1          1.285007           15.405691\n",
       "10                0          1.392544           15.405691\n",
       "11                1          3.259796           15.405691\n",
       "12                1          1.433205           15.405691\n",
       "13                0          1.280274           15.405691\n",
       "14                0          1.395333           15.405691\n",
       "15                0          1.175252           15.405691\n",
       "16                1          1.398108           15.405691\n",
       "17                1         10.723013           15.405691\n",
       "18                0          2.037565           15.405691\n",
       "19                0          1.503063           15.405691\n",
       "20                0          1.670454           15.405691\n",
       "21               12         10.042000           15.405691\n",
       "22                1          0.973671           15.405691\n",
       "23                0          1.550231           15.405691\n",
       "24                0          1.239094           15.405691\n",
       "25                0          0.745692           15.405691\n",
       "26                0          0.938713           15.405691\n",
       "27                0          4.339663           15.405691\n",
       "28                0          1.682943           15.405691\n",
       "29                0          1.427350           15.405691\n",
       "...             ...               ...                 ...\n",
       "6041              0          1.560628           15.405691\n",
       "6042              2          1.508284           15.405691\n",
       "6043              7          8.568823           15.405691\n",
       "6044              0          0.953897           15.405691\n",
       "6045              0          1.621893           15.405691\n",
       "6046              0          0.652921           15.405691\n",
       "6047              0          0.691062           15.405691\n",
       "6048              8          0.509152           15.405691\n",
       "6049              0          2.251305           15.405691\n",
       "6050              0          1.482157           15.405691\n",
       "6051              0          0.899055           15.405691\n",
       "6052              0          1.725007           15.405691\n",
       "6053              1          1.249538           15.405691\n",
       "6054              0          0.954063           15.405691\n",
       "6055              0          1.788742           15.405691\n",
       "6056              0          1.358596           15.405691\n",
       "6057              0          1.584856           15.405691\n",
       "6058              2          2.947666           15.405691\n",
       "6059              0          0.914681           15.405691\n",
       "6060              0          0.788576           15.405691\n",
       "6061              0          1.389155           15.405691\n",
       "6062              1          1.737977           15.405691\n",
       "6063             33         74.631020           15.405691\n",
       "6064              1          1.776015           15.405691\n",
       "6065              2          4.810035           15.405691\n",
       "6066              0          1.064351           15.405691\n",
       "6067              0          1.370239           15.405691\n",
       "6068              0          1.368928           15.405691\n",
       "6069              1          0.916928           15.405691\n",
       "6070              1          1.282302           15.405691\n",
       "\n",
       "[6071 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "y_pred_2 = regressor_2.predict(X_test)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred_2 = y_pred_2.reshape(6091,)\n",
    "temp_2 = {'Actual Values': y_test,'Predicted Values': y_pred_2}\n",
    "y_compare_2 = pd.DataFrame(temp_2)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "y_compare_2['Mean Squared Error'] = ((np.diff(y_compare_1.values) ** 2).mean() ** .5)\n",
    "y_compare_2.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
