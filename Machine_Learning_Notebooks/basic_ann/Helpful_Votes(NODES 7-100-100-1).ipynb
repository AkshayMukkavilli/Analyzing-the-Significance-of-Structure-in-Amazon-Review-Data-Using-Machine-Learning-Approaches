{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will now try to pridict the Actual Helpful Votes instead of the Z-scores of the helpful votes.\n",
    "\n",
    "1. With Z_Score_Words       ---->\n",
    "2. With Actual Words Count  ---->.  No Z_Score_HelpfulVotes in both cases.\n",
    "\n",
    "\n",
    "SCALER: MinMaxScaler\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are converting the dataset into two different variations:\n",
    "\n",
    "1. Keeping the Z_Score_Words and removing the Words column(dataset_z)\n",
    "2. Keeping Words and leaving Z_Score_Words(dataset_w)\n",
    "\n",
    "In both cases, we will not consider the z-score of helpful votes in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Z_Score_Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Helpful Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6.453577</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>3087.000000</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1.394079</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3.666459</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>468.500000</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.525083</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>394.272727</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.795826</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>91</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Z_Score_Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0      3       6.453577           1              0                      3   \n",
       "1      5       1.394079           3              4                      3   \n",
       "2      4       3.666459           4              6                      4   \n",
       "3      4       8.525083          11             20                      3   \n",
       "4      5       1.795826           2              1                      6   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Helpful Votes  \n",
       "0                     93                   3087.000000            837  \n",
       "1                     91                    300.000000            374  \n",
       "2                     90                    468.500000            263  \n",
       "3                     91                    394.272727            200  \n",
       "4                     91                    492.000000            247  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'../FinalFeatures.csv')\n",
    "\n",
    "dataset_z = dataset[['Stars','Z_Score_Words', 'Paragraphs','No.break tags','Percentage_Upper_Case','Percentage_Lower_Case','Avg_len_paragraph_per_review','Helpful Votes']]\n",
    "dataset_z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Helpful Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>565</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>3087.000000</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>343</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>468.500000</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>730</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>394.272727</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>194</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>91</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0      3    565           1              0                      3   \n",
       "1      5    162           3              4                      3   \n",
       "2      4    343           4              6                      4   \n",
       "3      4    730          11             20                      3   \n",
       "4      5    194           2              1                      6   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Helpful Votes  \n",
       "0                     93                   3087.000000            837  \n",
       "1                     91                    300.000000            374  \n",
       "2                     90                    468.500000            263  \n",
       "3                     91                    394.272727            200  \n",
       "4                     91                    492.000000            247  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_w = dataset[['Stars','Words', 'Paragraphs','No.break tags','Percentage_Upper_Case','Percentage_Lower_Case','Avg_len_paragraph_per_review','Helpful Votes']]\n",
    "dataset_w.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling both the datasets below using the MinMaxScaler from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Z_Score_Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Helpful Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.315262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.417478</td>\n",
       "      <td>0.167433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.091377</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.040449</td>\n",
       "      <td>0.074815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.191931</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.052611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.406928</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.040008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.109154</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.066423</td>\n",
       "      <td>0.049410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Z_Score_Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0   0.50       0.315262    0.000000       0.000000                   0.03   \n",
       "1   1.00       0.091377    0.015873       0.026846                   0.03   \n",
       "2   0.75       0.191931    0.023810       0.040268                   0.04   \n",
       "3   0.75       0.406928    0.079365       0.134228                   0.03   \n",
       "4   1.00       0.109154    0.007937       0.006711                   0.06   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Helpful Votes  \n",
       "0                   0.93                      0.417478       0.167433  \n",
       "1                   0.91                      0.040449       0.074815  \n",
       "2                   0.90                      0.063244       0.052611  \n",
       "3                   0.91                      0.053202       0.040008  \n",
       "4                   0.91                      0.066423       0.049410  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for dataset with z-score of words\n",
    "\n",
    "dataset_new_z = scaler.fit_transform(dataset_z)\n",
    "dataset_new_z = pd.DataFrame(dataset_new_z,columns=dataset_z.columns)\n",
    "dataset_new_z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Words</th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>No.break tags</th>\n",
       "      <th>Percentage_Upper_Case</th>\n",
       "      <th>Percentage_Lower_Case</th>\n",
       "      <th>Avg_len_paragraph_per_review</th>\n",
       "      <th>Helpful Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.211950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.417478</td>\n",
       "      <td>0.167433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.060504</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.040449</td>\n",
       "      <td>0.074815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.128523</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.052611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.273957</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.040008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.072529</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.066423</td>\n",
       "      <td>0.049410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars     Words  Paragraphs  No.break tags  Percentage_Upper_Case  \\\n",
       "0   0.50  0.211950    0.000000       0.000000                   0.03   \n",
       "1   1.00  0.060504    0.015873       0.026846                   0.03   \n",
       "2   0.75  0.128523    0.023810       0.040268                   0.04   \n",
       "3   0.75  0.273957    0.079365       0.134228                   0.03   \n",
       "4   1.00  0.072529    0.007937       0.006711                   0.06   \n",
       "\n",
       "   Percentage_Lower_Case  Avg_len_paragraph_per_review  Helpful Votes  \n",
       "0                   0.93                      0.417478       0.167433  \n",
       "1                   0.91                      0.040449       0.074815  \n",
       "2                   0.90                      0.063244       0.052611  \n",
       "3                   0.91                      0.053202       0.040008  \n",
       "4                   0.91                      0.066423       0.049410  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for dataset with just the words\n",
    "\n",
    "dataset_new_w = scaler.fit_transform(dataset_w)\n",
    "dataset_new_w = pd.DataFrame(dataset_new_w,columns=dataset_w.columns)\n",
    "dataset_new_w.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will split all the features from the target variable which is \"Helpful Votes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset with z-score of words\n",
    "\n",
    "X_z = dataset_new_z.iloc[:,0:-1].values\n",
    "y_z = dataset_new_z.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset with words\n",
    "\n",
    "X_w = dataset_new_w.iloc[:,0:-1].values\n",
    "y_w = dataset_new_w.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset with the z-score of words\n",
    "\n",
    "X_train_z,X_test_z,y_train_z,y_test_z = train_test_split(X_z,y_z, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset with words\n",
    "\n",
    "X_train_w,X_test_w,y_train_w,y_test_w = train_test_split(X_w,y_w, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use StandardScaler for scaling the data instead of MinMaxScaler, this code can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build the ANN for our regression task.\n",
    "\n",
    "Since we have already tried the ANN with less number of nodes and the relusts were not impressive, we will use a high number of nodes and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(Dense(100, kernel_initializer = 'uniform', activation = 'relu', input_dim = 7))\n",
    "regressor.add(Dense(100, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "regressor.add(Dense(1, kernel_initializer = 'uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dense class that we've imported above will help us initilize the neural network with small weights close to 0 but not 0\n",
    "\n",
    "We did not provide any activation function in the output layer since we are not classifying the data. For classification, we need to provide an activation function for the output layer, whereas we do not need to provide one for regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the regressor here\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have compiled the regressor, we can fit the Artificial Neural Network to out training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 8.5890e-05\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.3115e-05\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.5344e-05\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 8.0669e-05\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 8.4573e-05\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.4416e-05\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.5741e-05\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.4553e-05\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.4312e-05\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 8.3660e-05\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 7.9934e-05\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 8.5783e-05\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 7.9270e-05\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.9150e-05\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.0359e-05\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 8.2089e-05\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 7.9338e-05 0s - loss:\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.5332e-05\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 7.4616e-05\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.2928e-05 0s - l\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.5124e-05\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.4953e-05\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.4781e-05\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.4173e-05\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.4291e-05\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.2979e-05\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 8.3245e-05\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.3525e-05\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.1433e-05\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.3519e-05\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.2242e-05\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.2993e-05\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.3717e-05\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 8.0250e-05\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.0424e-05\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.3265e-05\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.1266e-05\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 8.1452e-05\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 8.0856e-05\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.1565e-05\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 8.2951e-05\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.0631e-05\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.2459e-05\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.1170e-05\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.0875e-05\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 8.1577e-05\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 7.9518e-05\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 7.8023e-05\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.0528e-05\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.0374e-05\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 8.3177e-05\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.1353e-05\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 7.9479e-05\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.7298e-05\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 8.0480e-05\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.9942e-05\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 8.0021e-05\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 8.0765e-05\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 7.7584e-05\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 7.8561e-05\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 8.4396e-05\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 7.5936e-05\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 7.9366e-05\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 7.9415e-05\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.1671e-05\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.8225e-05\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.5636e-05\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 7.7561e-05\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 8.3825e-05\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 7.8876e-05\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 8.4502e-05\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 7.8744e-05\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 4s 180us/step - loss: 7.8506e-05\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 7.7479e-05\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.9369e-05\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.7374e-05\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.0109e-05\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 7.7935e-05\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.7724e-05\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.0775e-05\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.9427e-05\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 8.0853e-05\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 7.7677e-05\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.7128e-05\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 7.6997e-05\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24362/24362 [==============================] - 4s 165us/step - loss: 7.6976e-05\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 7.8606e-05\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.7354e-05\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 7.6343e-05\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 7.6247e-05\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.1028e-05\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.2798e-05\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 7.7029e-05\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 7.7850e-05\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 7.6839e-05\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 8.0483e-05\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 7.8637e-05\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.0484e-05\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 7.5906e-05\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 7.6224e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135b33f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the ANN on the training dataset X_z( the one with z-score of words)\n",
    "\n",
    "regressor.fit(X_train_z, y_train_z, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6091,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (7,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e2022793ef43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/WorkingCopy/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (7,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\"\"\" Predicting the values for Helpful Votes on the test data \"\"\"\n",
    "print(y_test_z.shape)\n",
    "y_pred_z = regressor.predict(y_test_z)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against predicted values \"\"\"\n",
    "y_pred_z = y_pred_z.reshape(6091,)\n",
    "temp = {'Actual Values': y_test,'Predicted Values': y_pred_z}\n",
    "y_compare_z = pd.DataFrame(temp)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN\"\"\"\n",
    "# We are calculating this MSE in two steps. Don't get confused.\n",
    "y_compare_z['Mean Squared Error'] = (np.diff(y_compare_z.values) ** 2)\n",
    "y_compare_z['Mean Squared Error'] = np.mean(y_compare_z['Mean Squared Error'])\n",
    "\n",
    "# Now calculating the Root Mean Squared Error(RMSE)\n",
    "y_compare_z['Root Mean Squared Error'] = y_compare_z['Mean Squared Error']**0.5\n",
    "y_compare_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Now we will also predict the y values on the training set just to calculate MSE and RMSE \"\"\"\n",
    "\n",
    "y_pred_train_z = regressor.predict(X_train_z)\n",
    "\n",
    "\"\"\" Creating a dataframe to compare the actual values against the predicted values for the training set \"\"\"\n",
    "y_pred_train_z = y_pred_train_z.reshape(24362,)\n",
    "temp_train = {'Actual Values(Training)':y_train_z, 'Predicted Values(Training)': y_pred_train_z }\n",
    "y_compare_train_z = pd.DataFrame(temp_train)\n",
    "\n",
    "\"\"\" Calculating the Mean Squared Error to estimate the efficiency of the ANN on TRAINING SET\"\"\"\n",
    "# We are calculating this MSE in two steps. Don't get confused.\n",
    "y_compare_train_z['Mean Squared Error'] = (np.diff(y_compare_train_z.values) ** 2)\n",
    "y_compare_train_z['Mean Squared Error'] = np.mean(y_compare_train_z['Mean Squared Error'])\n",
    "\n",
    "# Now calculating the Root Mean Squared Error(RMSE)\n",
    "y_compare_train_z['Root Mean Squared Error'] = y_compare_train_z['Mean Squared Error']**0.5\n",
    "y_compare_train_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.6913e-05 0s - loss\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.5888e-05 \n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5427e-05\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 8.4904e-05 1s\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.6864e-05 0s - loss: 8.8620e-\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.6325e-05 0s - loss: 9.0169\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5826e-05\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.5983e-05\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.5813e-05 0s - loss: 8.7186e-\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5250e-05\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5776e-05\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 4s 152us/step - loss: 8.4294e-05\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.6291e-05- ETA: 0s - loss:\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.6238e-05\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5203e-05\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 8.5080e-05\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5931e-05\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.5431e-05\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.6357e-05\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.5531e-05\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 151us/step - loss: 8.5706e-05\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 151us/step - loss: 8.5318e-05\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 151us/step - loss: 8.5559e-05 0s -\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.4883e-05\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 4s 151us/step - loss: 8.5624e-05\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 151us/step - loss: 8.6703e-05\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 151us/step - loss: 8.5194e-05\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 153us/step - loss: 8.5552e-05\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 4s 152us/step - loss: 8.5032e-05 0s - loss:\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.5387e-05\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.5982e-05\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.5420e-05\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.5099e-05 1s - loss: 1.2192e - ETA:\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 8.5124e-05\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 4s 164us/step - loss: 8.6529e-05 0s - lo\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.6169e-05\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5654e-05\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 4s 166us/step - loss: 8.5151e-05\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4937e-05\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.5148e-05\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 4s 154us/step - loss: 8.5223e-05\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 154us/step - loss: 8.5350e-05\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 153us/step - loss: 8.5429e-05 0s - loss: 9.\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5501e-05\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 4s 154us/step - loss: 8.5227e-05\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 154us/step - loss: 8.4923e-05\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 154us/step - loss: 8.5291e-05\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5050e-05\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.5261e-05\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4932e-05\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5515e-05 0s - loss: 8.6923e-\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5147e-05\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.5811e-05 0\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4364e-05\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.5918e-05\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5358e-05 0s - loss: 8.7996e- - ETA: 0s - loss: 8.5696e-0\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.4843e-05\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.5244e-05 0s - loss: 9.674 - ETA: 0s - loss: 9.00\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.4649e-05\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.4794e-05\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5729e-05\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.4493e-05\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5002e-05\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.4048e-05\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.4586e-05\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.3892e-05\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.3517e-05\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5071e-05 0s - lo\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.5081e-05 0s\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5117e-05\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.4703e-05\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4347e-05\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5483e-05 - ETA: \n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.4763e-05 0s - loss: 8.7330e\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.5337e-05 1s - loss: 5.3571e- - ETA:\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4731e-05\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4405e-05\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.5149e-05\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.4777e-05\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 8.4424e-05\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.5463e-05\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4591e-05\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4777e-05\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.4132e-05\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 4s 149us/step - loss: 8.5295e-05\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 4s 147us/step - loss: 8.5291e-05\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 4s 150us/step - loss: 8.3399e-05\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 4s 150us/step - loss: 8.4917e-05\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.4616e-05\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.4321e-05 0s - lo\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.4623e-05 0s - loss\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.3477e-05\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.4858e-05\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 8.4060e-05  - ETA: 1s - lo - ETA: 0s - \n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.4327e-05\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 153us/step - loss: 8.4688e-05\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.3517e-05 0s - loss: 9.\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 153us/step - loss: 8.4835e-05\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 153us/step - loss: 8.4575e-05\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 4s 152us/step - loss: 8.4622e-05 1s - loss: 4. - ETA: 1s - loss: 3.4702e-0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15df6ec7320>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the ANN for the dataset with just the words\n",
    "\n",
    "regressor.fit(X_train_w, y_train_w, batch_size=5, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_w = regressor.predict(X_test_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a pandas dataframe with the actual values compared with the target variabes that have been predicted using both the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6091,)\n",
      "(6091,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Pred-Dataset with words only</th>\n",
       "      <th>Pred-Dataset with z-scores</th>\n",
       "      <th>MSE-&gt; z-score</th>\n",
       "      <th>MSE-&gt; just words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005598</td>\n",
       "      <td>-0.005598</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005475</td>\n",
       "      <td>-0.005475</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005396</td>\n",
       "      <td>-0.005396</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005301</td>\n",
       "      <td>-0.005301</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005349</td>\n",
       "      <td>-0.005349</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.004298</td>\n",
       "      <td>-0.004298</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005782</td>\n",
       "      <td>-0.005782</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005391</td>\n",
       "      <td>-0.005391</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005282</td>\n",
       "      <td>-0.005282</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005345</td>\n",
       "      <td>-0.005345</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.002232</td>\n",
       "      <td>-0.002232</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005129</td>\n",
       "      <td>-0.005129</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual Values  Pred-Dataset with words only  Pred-Dataset with z-scores  \\\n",
       "0          0.0002                     -0.005598                   -0.005598   \n",
       "1          0.0000                     -0.005475                   -0.005475   \n",
       "2          0.0000                     -0.005396                   -0.005396   \n",
       "3          0.0000                     -0.005301                   -0.005301   \n",
       "4          0.0000                     -0.005349                   -0.005349   \n",
       "5          0.0000                     -0.004298                   -0.004298   \n",
       "6          0.0000                     -0.005782                   -0.005782   \n",
       "7          0.0000                     -0.005392                   -0.005392   \n",
       "8          0.0000                     -0.005585                   -0.005585   \n",
       "9          0.0002                     -0.005406                   -0.005406   \n",
       "10         0.0000                     -0.005391                   -0.005391   \n",
       "11         0.0002                     -0.004461                   -0.004461   \n",
       "12         0.0002                     -0.005164                   -0.005164   \n",
       "13         0.0000                     -0.005282                   -0.005282   \n",
       "14         0.0000                     -0.005348                   -0.005348   \n",
       "15         0.0000                     -0.005525                   -0.005525   \n",
       "16         0.0002                     -0.005345                   -0.005345   \n",
       "17         0.0002                     -0.002232                   -0.002232   \n",
       "18         0.0000                     -0.005129                   -0.005129   \n",
       "19         0.0000                     -0.005440                   -0.005440   \n",
       "\n",
       "    MSE-> z-score  MSE-> just words  \n",
       "0        0.006071          0.006071  \n",
       "1        0.006071          0.006071  \n",
       "2        0.006071          0.006071  \n",
       "3        0.006071          0.006071  \n",
       "4        0.006071          0.006071  \n",
       "5        0.006071          0.006071  \n",
       "6        0.006071          0.006071  \n",
       "7        0.006071          0.006071  \n",
       "8        0.006071          0.006071  \n",
       "9        0.006071          0.006071  \n",
       "10       0.006071          0.006071  \n",
       "11       0.006071          0.006071  \n",
       "12       0.006071          0.006071  \n",
       "13       0.006071          0.006071  \n",
       "14       0.006071          0.006071  \n",
       "15       0.006071          0.006071  \n",
       "16       0.006071          0.006071  \n",
       "17       0.006071          0.006071  \n",
       "18       0.006071          0.006071  \n",
       "19       0.006071          0.006071  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_z=y_pred_z.reshape(6091,)\n",
    "y_pred_w=y_pred_w.reshape(6091,)\n",
    "print(y_pred_z.shape)\n",
    "print(y_pred_w.shape)\n",
    "temp={'Actual Values': y_test_z,'Pred-Dataset with z-scores': y_pred_z,'Pred-Dataset with words only': y_pred_w}\n",
    "y_compare = pd.DataFrame(temp)\n",
    "\n",
    "# Calculating the Mean Squared Error and adding the columns with the MSE\n",
    "y_compare['MSE-> z-score'] = (((y_compare['Actual Values'].values - y_compare['Pred-Dataset with z-scores'].values ) ** 2).mean() ** .5)\n",
    "y_compare['MSE-> just words'] = (((y_compare['Actual Values'].values - y_compare['Pred-Dataset with words only'].values ) ** 2).mean() ** .5)\n",
    "y_compare.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual values and the predicted values are way too far from each other in both the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try a regressor  with a different activation function and check if it helps in any way\n",
    "\n",
    "Activation Function: Leaky ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "# Trying with different number of nodes as well\n",
    "regressor_leaky = Sequential()\n",
    "regressor_leaky.add(Dense(75, kernel_initializer = 'normal',input_dim = 7))\n",
    "regressor_leaky.add(LeakyReLU(alpha=0.05))\n",
    "regressor_leaky.add(Dense(75, kernel_initializer= 'normal'))\n",
    "regressor_leaky.add(LeakyReLU(alpha=0.05))\n",
    "regressor_leaky.add(Dense(50, kernel_initializer= 'normal'))\n",
    "regressor_leaky.add(LeakyReLU(alpha=0.05))\n",
    "regressor_leaky.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "# Compiling the regressor\n",
    "regressor_leaky.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.6120e-05 0s - loss: 9.3\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 8.5771e-05\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 8.5966e-05\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 4s 170us/step - loss: 8.4581e-05\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.5088e-05\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.2320e-05\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 7.8229e-05\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.2078e-05 \n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 8.5297e-05 1s\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 7.9097e-05 0s - loss: \n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.5758e-05\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.3967e-05\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 8.6416e-05 3s - - ETA: 2s - loss: 5.76 \n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.5371e-05: 0\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 4s 180us/step - loss: 8.4465e-05 0s - los - ETA: 0s - loss: 9\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.4251e-05\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.2775e-05\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 8.2632e-05\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.5446e-05 1s - loss:  - ETA: 0s - loss\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 178us/step - loss: 8.3009e-05\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.4818e-05 0\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.1806e-05 0s - loss: 7.386\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.3506e-05\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 172us/step - loss: 8.5468e-05 0s - loss: 8.63\n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.1678e-05 0s - loss\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 8.5174e-05\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.3404e-05\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.4809e-05\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 5s 188us/step - loss: 7.9398e-05 2s - loss: 1 - ETA\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 183us/step - loss: 8.5624e-05\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.1587e-05\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 5s 188us/step - loss: 8.2615e-05\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 8.2318e-05\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 8.0588e-05\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.1402e-05\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 5s 188us/step - loss: 8.1049e-05 \n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.4312e-05\n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 4s 183us/step - loss: 8.4926e-05\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.4565e-05\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.2121e-05\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 8.3531e-05\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 177us/step - loss: 8.0648e-05\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 8.4077e-05\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.0448e-05 2s - \n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 8.0537e-05\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 7.5946e-05\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 183us/step - loss: 8.4353e-05\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.5361e-05\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.2703e-05\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 7.8512e-05\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 5s 195us/step - loss: 8.4479e-05 1s - loss: 9.7847\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 5s 191us/step - loss: 8.3742e-05\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 8.3605e-05\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 5s 194us/step - loss: 8.3950e-05\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 8.2937e-05\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 4s 179us/step - loss: 8.2774e-05\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 5s 198us/step - loss: 8.3780e-05 0s - loss\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 9.1837e-05 0s - los\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 5s 201us/step - loss: 8.1746e-05\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 5s 185us/step - loss: 8.2902e-05\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 8.2313e-05\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 8.0863e-05\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 5s 197us/step - loss: 8.2763e-05\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 4s 183us/step - loss: 8.1295e-05\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 5s 191us/step - loss: 8.2842e-05\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.1544e-05\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 5s 189us/step - loss: 8.1328e-05\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.2369e-05\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.1743e-05\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 5s 193us/step - loss: 9.0474e-05\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 8.2143e-05 \n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.0225e-05\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 5s 185us/step - loss: 8.2060e-05\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 5s 185us/step - loss: 8.0072e-05\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 8.2736e-05 1s - loss: 7.8036e-0 - ETA: 1s  - ETA: 0s - loss: 1. - ETA: 0s - loss: \n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 8.1218e-05\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 5s 185us/step - loss: 8.0466e-05\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 184us/step - loss: 8.1050e-05\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 5s 191us/step - loss: 7.9171e-05\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 5s 193us/step - loss: 8.3374e-05\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 5s 199us/step - loss: 8.1738e-05\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 8.2059e-05\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 5s 195us/step - loss: 8.0125e-05\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 7.8776e-05\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.1061e-05\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 4s 175us/step - loss: 7.9305e-05\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 7.9916e-05\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.6722e-05 0s - \n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.1310e-05\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.0854e-05\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.0797e-05\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.9384e-05\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.9083e-05\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.9624e-05\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 7.9404e-05\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.0207e-05\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.0258e-05\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 7.9178e-05\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.0819e-05\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 5s 195us/step - loss: 7.8212e-05\n"
     ]
    }
   ],
   "source": [
    "# Fitting the ANN on training dataset with z-scores and then predicting the values for test dataset\n",
    "regressor_leaky.fit(X_train_z, y_train_z, batch_size=5, epochs=100, verbose = 1)\n",
    "y_pred_leaky_z = regressor_leaky.predict(X_test_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24362/24362 [==============================] - 5s 200us/step - loss: 8.7448e-05\n",
      "Epoch 2/100\n",
      "24362/24362 [==============================] - 5s 185us/step - loss: 8.8351e-05\n",
      "Epoch 3/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 8.6220e-05\n",
      "Epoch 4/100\n",
      "24362/24362 [==============================] - 5s 186us/step - loss: 8.6494e-05\n",
      "Epoch 5/100\n",
      "24362/24362 [==============================] - 5s 189us/step - loss: 8.6706e-05 0s\n",
      "Epoch 6/100\n",
      "24362/24362 [==============================] - 5s 189us/step - loss: 8.6958e-05\n",
      "Epoch 7/100\n",
      "24362/24362 [==============================] - 5s 188us/step - loss: 8.6519e-05\n",
      "Epoch 8/100\n",
      "24362/24362 [==============================] - 5s 194us/step - loss: 8.6435e-05\n",
      "Epoch 9/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 8.5375e-05\n",
      "Epoch 10/100\n",
      "24362/24362 [==============================] - 4s 181us/step - loss: 8.5992e-05 1s - - ETA: 0s - lo\n",
      "Epoch 11/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 8.4763e-05\n",
      "Epoch 12/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 8.7494e-05\n",
      "Epoch 13/100\n",
      "24362/24362 [==============================] - 5s 187us/step - loss: 8.5042e-05\n",
      "Epoch 14/100\n",
      "24362/24362 [==============================] - 4s 182us/step - loss: 8.3688e-05\n",
      "Epoch 15/100\n",
      "24362/24362 [==============================] - 6s 226us/step - loss: 8.5097e-05\n",
      "Epoch 16/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.5024e-05 0s -\n",
      "Epoch 17/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.4379e-05\n",
      "Epoch 18/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.5203e-05 0s - loss: 8.996\n",
      "Epoch 19/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.4435e-05 0s - loss: 8.\n",
      "Epoch 20/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.4299e-05\n",
      "Epoch 21/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.3893e-05\n",
      "Epoch 22/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.3861e-05\n",
      "Epoch 23/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.4526e-05 0s - loss: 8.608\n",
      "Epoch 24/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 8.3789e-05 0s - loss: 8.41 - ETA: 0s - loss: \n",
      "Epoch 25/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 8.3720e-05\n",
      "Epoch 26/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.5273e-05\n",
      "Epoch 27/100\n",
      "24362/24362 [==============================] - 4s 161us/step - loss: 8.4086e-05\n",
      "Epoch 28/100\n",
      "24362/24362 [==============================] - 4s 159us/step - loss: 8.5091e-05\n",
      "Epoch 29/100\n",
      "24362/24362 [==============================] - 4s 176us/step - loss: 8.3754e-05\n",
      "Epoch 30/100\n",
      "24362/24362 [==============================] - 4s 171us/step - loss: 8.3921e-05 0s - loss: 8.5748e-\n",
      "Epoch 31/100\n",
      "24362/24362 [==============================] - 4s 163us/step - loss: 8.3467e-05 0s -  - ETA: 0s - loss: 8.5497e\n",
      "Epoch 32/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.4184e-05\n",
      "Epoch 33/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.4233e-05\n",
      "Epoch 34/100\n",
      "24362/24362 [==============================] - 4s 167us/step - loss: 8.5723e-05\n",
      "Epoch 35/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.3457e-05\n",
      "Epoch 36/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.4676e-05\n",
      "Epoch 37/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.2708e-05 0s \n",
      "Epoch 38/100\n",
      "24362/24362 [==============================] - 5s 193us/step - loss: 8.4395e-05\n",
      "Epoch 39/100\n",
      "24362/24362 [==============================] - 6s 241us/step - loss: 8.3600e-05\n",
      "Epoch 40/100\n",
      "24362/24362 [==============================] - 6s 229us/step - loss: 8.4523e-05\n",
      "Epoch 41/100\n",
      "24362/24362 [==============================] - 5s 224us/step - loss: 8.4524e-05\n",
      "Epoch 42/100\n",
      "24362/24362 [==============================] - 4s 160us/step - loss: 8.3644e-05\n",
      "Epoch 43/100\n",
      "24362/24362 [==============================] - 4s 145us/step - loss: 8.3307e-05\n",
      "Epoch 44/100\n",
      "24362/24362 [==============================] - 4s 144us/step - loss: 8.3149e-05\n",
      "Epoch 45/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 8.7247e-05 0s - loss:\n",
      "Epoch 46/100\n",
      "24362/24362 [==============================] - 4s 149us/step - loss: 8.2319e-05\n",
      "Epoch 47/100\n",
      "24362/24362 [==============================] - 4s 149us/step - loss: 8.5190e-05\n",
      "Epoch 48/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.3556e-05\n",
      "Epoch 49/100\n",
      "24362/24362 [==============================] - 4s 146us/step - loss: 8.2227e-05 1s -  - ET\n",
      "Epoch 50/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.4422e-05\n",
      "Epoch 51/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 8.6026e-05\n",
      "Epoch 52/100\n",
      "24362/24362 [==============================] - 4s 144us/step - loss: 8.3286e-05\n",
      "Epoch 53/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.2984e-05\n",
      "Epoch 54/100\n",
      "24362/24362 [==============================] - 3s 144us/step - loss: 8.4688e-05 0s - loss: 8.5271e-0\n",
      "Epoch 55/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 8.5137e-05\n",
      "Epoch 56/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.2800e-05  - ETA: 0s - loss\n",
      "Epoch 57/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.1913e-05\n",
      "Epoch 58/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.4710e-05\n",
      "Epoch 59/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.3264e-05\n",
      "Epoch 60/100\n",
      "24362/24362 [==============================] - 3s 144us/step - loss: 8.5971e-05\n",
      "Epoch 61/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.2884e-05\n",
      "Epoch 62/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.5239e-05\n",
      "Epoch 63/100\n",
      "24362/24362 [==============================] - 4s 144us/step - loss: 8.3245e-05\n",
      "Epoch 64/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.3348e-05\n",
      "Epoch 65/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.4226e-05\n",
      "Epoch 66/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.3892e-05 0\n",
      "Epoch 67/100\n",
      "24362/24362 [==============================] - 4s 169us/step - loss: 8.2697e-05\n",
      "Epoch 68/100\n",
      "24362/24362 [==============================] - 4s 144us/step - loss: 8.4188e-05\n",
      "Epoch 69/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 8.2677e-05\n",
      "Epoch 70/100\n",
      "24362/24362 [==============================] - 4s 148us/step - loss: 8.4113e-05\n",
      "Epoch 71/100\n",
      "24362/24362 [==============================] - 4s 155us/step - loss: 8.5034e-05\n",
      "Epoch 72/100\n",
      "24362/24362 [==============================] - 4s 152us/step - loss: 8.2305e-05\n",
      "Epoch 73/100\n",
      "24362/24362 [==============================] - 4s 156us/step - loss: 8.2526e-05\n",
      "Epoch 74/100\n",
      "24362/24362 [==============================] - 4s 150us/step - loss: 8.3494e-05 2 - ETA: 0s - loss: 8.5413e-\n",
      "Epoch 75/100\n",
      "24362/24362 [==============================] - 4s 147us/step - loss: 8.2816e-05 0s -\n",
      "Epoch 76/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 8.5122e-05\n",
      "Epoch 77/100\n",
      "24362/24362 [==============================] - 4s 151us/step - loss: 8.4548e-05\n",
      "Epoch 78/100\n",
      "24362/24362 [==============================] - 4s 145us/step - loss: 8.3389e-05\n",
      "Epoch 79/100\n",
      "24362/24362 [==============================] - 4s 147us/step - loss: 8.3258e-05ETA: 0s - loss: - ETA: 0s - loss: 8.\n",
      "Epoch 80/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.3866e-05\n",
      "Epoch 81/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.2750e-05 0s\n",
      "Epoch 82/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.4206e-05\n",
      "Epoch 83/100\n",
      "24362/24362 [==============================] - 3s 144us/step - loss: 8.4867e-05 0s - loss: 3. - ETA: 0s - loss\n",
      "Epoch 84/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.2613e-05\n",
      "Epoch 85/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.2544e-05\n",
      "Epoch 86/100\n",
      "24362/24362 [==============================] - 4s 145us/step - loss: 8.9163e-05\n",
      "Epoch 87/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 8.2215e-05\n",
      "Epoch 88/100\n",
      "24362/24362 [==============================] - 3s 142us/step - loss: 8.3842e-05\n",
      "Epoch 89/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.3646e-05 0s\n",
      "Epoch 90/100\n",
      "24362/24362 [==============================] - 3s 143us/step - loss: 8.3663e-05 2\n",
      "Epoch 91/100\n",
      "24362/24362 [==============================] - 4s 148us/step - loss: 8.4834e-05\n",
      "Epoch 92/100\n",
      "24362/24362 [==============================] - 4s 174us/step - loss: 8.0134e-05\n",
      "Epoch 93/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.5397e-05\n",
      "Epoch 94/100\n",
      "24362/24362 [==============================] - 4s 158us/step - loss: 8.2830e-05\n",
      "Epoch 95/100\n",
      "24362/24362 [==============================] - 4s 162us/step - loss: 8.3778e-05\n",
      "Epoch 96/100\n",
      "24362/24362 [==============================] - 4s 157us/step - loss: 8.4324e-05\n",
      "Epoch 97/100\n",
      "24362/24362 [==============================] - 4s 165us/step - loss: 8.5021e-05 1s - loss: 8.987 - ETA: 1s - loss: 1 - ETA: 0s - los\n",
      "Epoch 98/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.3128e-05\n",
      "Epoch 99/100\n",
      "24362/24362 [==============================] - 4s 168us/step - loss: 8.4495e-05\n",
      "Epoch 100/100\n",
      "24362/24362 [==============================] - 4s 173us/step - loss: 8.3879e-05\n"
     ]
    }
   ],
   "source": [
    "# Fitting the ANN on training dataset with just words and then predicting the values for test dataset\n",
    "regressor_leaky.fit(X_train_w, y_train_w, batch_size=5, epochs=100, verbose = 1)\n",
    "y_pred_leaky_w = regressor_leaky.predict(X_test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Values</th>\n",
       "      <th>Leaky:Pred-Dataset with words only</th>\n",
       "      <th>Leaky:Pred-Dataset with z-scores</th>\n",
       "      <th>MSE-&gt; z-score</th>\n",
       "      <th>MSE-&gt; just words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005598</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005475</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005396</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005301</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005349</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.004298</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005782</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005391</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005282</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.005345</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.002232</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005129</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.006071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual Values  Leaky:Pred-Dataset with words only  \\\n",
       "0          0.0002                           -0.005598   \n",
       "1          0.0000                           -0.005475   \n",
       "2          0.0000                           -0.005396   \n",
       "3          0.0000                           -0.005301   \n",
       "4          0.0000                           -0.005349   \n",
       "5          0.0000                           -0.004298   \n",
       "6          0.0000                           -0.005782   \n",
       "7          0.0000                           -0.005392   \n",
       "8          0.0000                           -0.005585   \n",
       "9          0.0002                           -0.005406   \n",
       "10         0.0000                           -0.005391   \n",
       "11         0.0002                           -0.004461   \n",
       "12         0.0002                           -0.005164   \n",
       "13         0.0000                           -0.005282   \n",
       "14         0.0000                           -0.005348   \n",
       "15         0.0000                           -0.005525   \n",
       "16         0.0002                           -0.005345   \n",
       "17         0.0002                           -0.002232   \n",
       "18         0.0000                           -0.005129   \n",
       "19         0.0000                           -0.005440   \n",
       "\n",
       "    Leaky:Pred-Dataset with z-scores  MSE-> z-score  MSE-> just words  \n",
       "0                          -0.000260       0.005217          0.006071  \n",
       "1                          -0.000215       0.005217          0.006071  \n",
       "2                          -0.000217       0.005217          0.006071  \n",
       "3                           0.000413       0.005217          0.006071  \n",
       "4                          -0.000015       0.005217          0.006071  \n",
       "5                           0.001075       0.005217          0.006071  \n",
       "6                          -0.000058       0.005217          0.006071  \n",
       "7                          -0.000100       0.005217          0.006071  \n",
       "8                           0.000130       0.005217          0.006071  \n",
       "9                          -0.000303       0.005217          0.006071  \n",
       "10                          0.000035       0.005217          0.006071  \n",
       "11                          0.000615       0.005217          0.006071  \n",
       "12                         -0.000062       0.005217          0.006071  \n",
       "13                          0.000029       0.005217          0.006071  \n",
       "14                         -0.000264       0.005217          0.006071  \n",
       "15                         -0.000204       0.005217          0.006071  \n",
       "16                         -0.000087       0.005217          0.006071  \n",
       "17                          0.003704       0.005217          0.006071  \n",
       "18                          0.000722       0.005217          0.006071  \n",
       "19                         -0.000349       0.005217          0.006071  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_leaky_z=y_pred_leaky_z.reshape(6091,)\n",
    "y_pred_leaky_w=y_pred_leaky_w.reshape(6091,)\n",
    "\n",
    "temp={'Actual Values': y_test_z,'Leaky:Pred-Dataset with z-scores': y_pred_leaky_z,'Leaky:Pred-Dataset with words only': y_pred_w}\n",
    "y_compare_leaky = pd.DataFrame(temp)\n",
    "\n",
    "# Calculating the Mean Squared Error and adding the columns with the MSE\n",
    "y_compare_leaky['MSE-> z-score'] = (((y_compare_leaky['Actual Values'].values - y_compare_leaky['Leaky:Pred-Dataset with z-scores'].values ) ** 2).mean() ** .5)\n",
    "y_compare_leaky['MSE-> just words'] = (((y_compare_leaky['Actual Values'].values - y_compare_leaky['Leaky:Pred-Dataset with words only'].values ) ** 2).mean() ** .5)\n",
    "y_compare_leaky.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same case with a different activation function and a different number of nodes.\n",
    "\n",
    "Lower number of nodes has given us far superior results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
